{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final submission.ipynb","provenance":[{"file_id":"1rbYrpJjwLJmOsnT_moeZl8S9gKs0wbhe","timestamp":1619840858276},{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/classify_text_with_bert.ipynb","timestamp":1617745242590}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UndjqMB0Cure"},"source":["# Email Parsing for Relevant Information Using BERT"]},{"cell_type":"markdown","metadata":{"id":"Cb4espuLKJiA"},"source":[" Adopted from https://www.tensorflow.org/tutorials/text/classify_text_with_bert\n"]},{"cell_type":"markdown","metadata":{"id":"IZ6SNYq_tVVC"},"source":["This notebook contains complete code to preprocess and to train the classification network with BERT to decide which phrases are irrelevant in an email. This is the final product for Extract Relevant Data from Raw Email project for UC Berkeley's Data Science Discovery Program Sp'21.\n","\n","In this notebook code for the following is contained:\n","\n","- Preprocess the data\n","- Load a BERT model from TensorFlow Hub\n","- Build a model by combining BERT with a classifier\n","\n"]},{"cell_type":"markdown","metadata":{"id":"o1T7J-nhzlwO"},"source":["### Special Notes\n","\n","If you want to preprocess, annotate data and train the network, please run and follow instructions under: \n","1. Utility functions\n","2. Setup\n","3. Preprocess\n","4. Creating and Training Model\n","\n","If you just want to use a trained model to filter emails, please run and follow instructions under: \n","1. Utility functions\n","2. Setup\n","3. Using Model to Clean Email\n","\n","\n","*Unfortunately so far, we are only able to train using the sentences themselves, and not with any additional features avaliable in the dataset*\n","\n","*In addition if there are any module import error, please check https://www.tensorflow.org/tutorials/text/classify_text_with_bert for updates on which libraries to import*"]},{"cell_type":"markdown","metadata":{"id":"SCjmX4zTCkRK"},"source":["## Setup\n"]},{"cell_type":"code","metadata":{"id":"q-YbjCkzw0yU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621355300910,"user_tz":420,"elapsed":15751,"user":{"displayName":"Andrew Sue","photoUrl":"","userId":"12996359075612494384"}},"outputId":"47e8b0cd-9c5d-40aa-f5c3-398b0b33220f"},"source":["# A dependency of the preprocessing for BERT inputs\n","!pip install tf-nightly\n","!pip install -q tensorflow-text-nightly\n","!pip install -q tf-models-official"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tf-nightly in /usr/local/lib/python3.7/dist-packages (2.6.0.dev20210518)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.12.1)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.36.2)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.7.4.3)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.2.0)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.12)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.12.0)\n","Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.1.0)\n","Collecting grpcio<2.0,>=1.37.0\n","  Using cached https://files.pythonhosted.org/packages/31/d8/1bfe90cc49c166dd2ec1be46fa4830c254ce702004a110830c74ec1df0c0/grpcio-1.37.1-cp37-cp37m-manylinux2014_x86_64.whl\n","Collecting keras-nightly~=2.6.0.dev\n","  Using cached https://files.pythonhosted.org/packages/b1/f9/9366cd47fc47f2a2910881b4209864c0b08e7238c7ea568447322a88cefc/keras_nightly-2.6.0.dev2021051800-py2.py3-none-any.whl\n","Requirement already satisfied: tf-estimator-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (2.5.0.dev2021032601)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.19.5)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.4.0)\n","Requirement already satisfied: tb-nightly~=2.6.0.a in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (2.6.0a20210517)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.15.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.1.2)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.1.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.6.3)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.12.4)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.3.0)\n","Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tf-nightly) (1.5.2)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (56.1.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (3.3.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (1.8.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (0.4.4)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (0.6.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (2.0.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (1.30.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly~=2.6.0.a->tf-nightly) (4.0.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (2.10)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.6.0.a->tf-nightly) (1.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (4.2.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly~=2.6.0.a->tf-nightly) (3.4.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.6.0.a->tf-nightly) (3.1.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (0.4.8)\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement grpcio~=1.34.0, but you'll have grpcio 1.37.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement keras-nightly~=2.5.0.dev, but you'll have keras-nightly 2.6.0.dev2021051800 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow-text 2.4.3 has requirement tensorflow<2.5,>=2.4.0, but you'll have tensorflow 2.5.0 which is incompatible.\u001b[0m\n","Installing collected packages: grpcio, keras-nightly\n","  Found existing installation: grpcio 1.34.1\n","    Uninstalling grpcio-1.34.1:\n","      Successfully uninstalled grpcio-1.34.1\n","  Found existing installation: keras-nightly 2.5.0.dev2021032900\n","    Uninstalling keras-nightly-2.5.0.dev2021032900:\n","      Successfully uninstalled keras-nightly-2.5.0.dev2021032900\n","Successfully installed grpcio-1.37.1 keras-nightly-2.6.0.dev2021051800\n","\u001b[31mERROR: tf-nightly 2.6.0.dev20210518 has requirement grpcio<2.0,>=1.37.0, but you'll have grpcio 1.34.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tf-nightly 2.6.0.dev20210518 has requirement keras-nightly~=2.6.0.dev, but you'll have keras-nightly 2.5.0.dev2021032900 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow-text 2.4.3 has requirement tensorflow<2.5,>=2.4.0, but you'll have tensorflow 2.5.0 which is incompatible.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_XgTpm9ZxoN9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621355306943,"user_tz":420,"elapsed":6005,"user":{"displayName":"Andrew Sue","photoUrl":"","userId":"12996359075612494384"}},"outputId":"7b87af32-8992-4966-8f6a-af1345af35a4"},"source":["import os\n","import shutil\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from official.nlp import optimization  # to create AdamW optmizer\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers.experimental import preprocessing\n","\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","from spacy.lang.en import English\n","from spacy.lang.en.stop_words import STOP_WORDS\n","import spacy\n","import re\n","\n","nlp = spacy.load('en_core_web_sm')\n","## Run below for better accuracy and on local machine\n","# nlp = spacy.load('en_core_web_trf) \n","\n","tf.get_logger().setLevel('ERROR')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:43: UserWarning: You are currently using a nightly version of TensorFlow (2.6.0-dev20210518). \n","TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n","If you encounter a bug, do not file an issue on GitHub.\n","  UserWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"5bGeWW_wD-Mh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621355352992,"user_tz":420,"elapsed":25648,"user":{"displayName":"Andrew Sue","photoUrl":"","userId":"12996359075612494384"}},"outputId":"988c5a32-9dd6-4972-863b-6193aaa78c6b"},"source":["# run this cell only if you are on google colabs\n","from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UUJ3cwuUQ2Rn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621355353888,"user_tz":420,"elapsed":375,"user":{"displayName":"Andrew Sue","photoUrl":"","userId":"12996359075612494384"}},"outputId":"b7d4b2ff-d14f-4f21-cd3f-e23ea3cde85a"},"source":["# cd to the directory where this notebook is located in your drive\n","%cd /gdrive/My\\ Drive/Colab\\ Notebooks/DSDP"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/gdrive/My Drive/Colab Notebooks/DSDP\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Gmkk1LGaHJOe"},"source":["## Utility functions"]},{"cell_type":"code","metadata":{"id":"XAybfBmgHItx"},"source":["def print_file(fname):\n","    with open(fname) as file:\n","        body = file.read()\n","    print(body)\n","\n","def read_email(fname):\n","    with open(fname, 'r') as email:\n","        text = email.read()\n","    return text\n","\n","def create_dir(dir):\n","    if not os.path.exists(dir):\n","        os.makedirs(dir)\n","        print(\"Created Directory : \", dir)\n","    else:\n","        print(\"Directory already existed : \", dir)\n","    return dir"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXh9cyRmHjvP"},"source":["# filter To:, Subject:, etc. Rough filter \n","def rough_check(paragraph):\n","    # Words that denotes header/footer\n","    flags = ['To: ', 'Subject: ', 'cc: ', 'Sent by: ']\n","    for f in flags:\n","        if f in paragraph:\n","            return False\n","    return True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ef0xnL5hH5SF"},"source":["# filter things within lines (fine filter)\n","def fine_check(line):\n","    flags = ['From: ', '-----', 'Dear', 'Sincerely, ', 'Best Regards, ', 'Re: ', '---', '___', '> > ','>> ']\n","    cleaned_line = line.strip()\n","\n","    if cleaned_line.isspace() or not cleaned_line:\n","        return False\n","\n","    for f in flags:\n","        if f in cleaned_line:\n","            return False\n","    return True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5LSxOWPDIPBx"},"source":["# Clean document and writes to outputs folder\n","def clean_summarize(file_name, fname, annotation=True):\n","    main_text = read_email(file_name).strip()\n","    paragraphs = main_text.split(sep='\\n\\n')[1:]\n","    content = list(filter(rough_check, paragraphs))\n","\n","    cleaned_content = []\n","    for p in content:\n","        lines = p.splitlines()\n","        cleaned_content.append('\\n'.join(list(filter(fine_check, lines))))\n","\n","    output_file = fname + '_out'\n","    output_text = ''.join(cleaned_content)\n","    output_text = ''.join(output_text.splitlines())\n","    doc = nlp(output_text)\n","    output_text = ''\n","    for sent in list(doc.sents):\n","        output_text += str(sent) + '\\n'\n","\n","    if annotation:\n","        with open(output_path_cleaned + output_file + '.txt', \"w\") as new_file:\n","            new_file.write(output_text)\n","\n","        with open(output_path_manual + output_file + '.txt', \"w\") as new_file:\n","            new_file.write(output_text)\n","    return output_text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1rJaMdZbFX9R"},"source":["## Preprocess"]},{"cell_type":"markdown","metadata":{"id":"MmbrOIC3LIFY"},"source":["### What it does\n","\n","*Filtering*\n","- Essentially, it does a filter through the emails and get rid of obvious unwanted information such as the header and tail and lines with words such as \"forwarded\" etc\n","- outputs 2 copies the cleaned file into 2 different directories for annotation, one as the original for comparison, the other to be altered.\n","\n","*Annotating*\n","- To annotate you go into one of the directories with the output to be altered and delete the lines you think are useless\n","- The script compare the files between the two directories and if a certain sentence is in the file of one directory, but not in the other, it gets a label of 0 (which means the line is useless) and vice versa.\n","- The script then computes the features for each sentence (position of the sentence in relation to the email, length of the sentence, etc.)\n","- All the labels get stored in a CSV associated with each file, and all the file's csv gets concatenated into a large csv as the input to the neural network"]},{"cell_type":"markdown","metadata":{"id":"hAIcYuk2JJh0"},"source":["### Instructions\n","1. Define where the inputs and outputs will reside: \n","    - ```outputs_path``` where the outputs resides \n","    - ```inputs_path``` where all the raw input email files to cleaned resides \n","2. Run the code below until \"Instructions to Annotate\"\n","3. Follow instructions under \"Instructions to Annotate\""]},{"cell_type":"markdown","metadata":{"id":"KZwnuP6fKqs7"},"source":["### Code to Generate Data to Label"]},{"cell_type":"code","metadata":{"id":"M8Hr9Yx6KCAj","colab":{"base_uri":"https://localhost:8080/","height":107},"executionInfo":{"status":"ok","timestamp":1621355503516,"user_tz":420,"elapsed":599,"user":{"displayName":"Andrew Sue","photoUrl":"","userId":"12996359075612494384"}},"outputId":"880253e3-484d-4fbf-960c-8d66e0b21546"},"source":["# Path for the directory of data to be processed\n","inputs_path = './Inputs/arora-h/'\n","outputs_path = './outputs'\n","\n","\n","# Path for cleaned outputs\n","output_path_cleaned = outputs_path + '/original/'\n","output_path_manual = outputs_path + '/alter/'\n","\n","# TODO: Generate necessary file structure\n","create_dir(output_path_cleaned)\n","create_dir(output_path_manual)\n","create_dir(outputs_path + '/labels')\n","create_dir(output_path_manual + 'done')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Directory already existed :  ./outputs/original/\n","Directory already existed :  ./outputs/alter/\n","Directory already existed :  ./outputs/labels\n","Directory already existed :  ./outputs/alter/done\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'./outputs/alter/done'"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"TbH3R4UOJUuz"},"source":["# cleans all file in the inputs directory\n","for files in os.listdir(inputs_path)[1:]:\n","    clean_summarize(inputs_path + files, files)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ctvC52LK6eR"},"source":["### Instructions to Annotate\n","1. Go to your ```<outputs_path>/alter``` directory\n","2. Open any file and delete ANY WHOLE LINES that you think does not contain necessary information for the email. Remember to delete the whole line!!!! Not just the part in the line you think is trivial.\n","3. Move all the files in which you have changed into the ```<outputs_path>/done``` directory\n","4. Run the cell that says ***Run this to get labels and features!*** below, your labeled data will be in a CSV file residing in ```<outputs_path>/labels``` directory\n","5. After every session of annotating or when you are done with annotating and you want to train the network with your labeled data, run the cell that says ***Run code cell below once all emails are annotated*** to concatanate all your labels in one CSV to be fed into the network. You can see the CSV in your ```outputs_path``` directory named  ```all_labeled_data.csv```\n","6. Proceed to create and train your network."]},{"cell_type":"markdown","metadata":{"id":"GYJWBR1pLVv4"},"source":["**Run this to get labels and features!**"]},{"cell_type":"code","metadata":{"id":"YaC99vA7J9Vy"},"source":["for fname in os.listdir(output_path_manual + '/done')[1:]:\n","    original = read_file(output_path_cleaned + fname).splitlines()\n","    edited = read_file(output_path_manual + '/done/'+fname).splitlines()\n","    labeled = []\n","\n","    for ln in original:\n","        if ln in edited:\n","            labeled += [[ln, 1]]\n","        else:\n","            labeled += [[ln, 0]]\n","    data = pd.DataFrame(labeled, columns=['Sentence', 'Label'])\n","    \n","    #positions of the sentences\n","    pos__sentence = []\n","    \n","    for i in range(len(original)):\n","        pos__sentence.append(i)\n","\n","            \n","    #length of the sentences\n","    len_sentence = []\n","    \n","    for line in original:\n","        len_sentence.append(len(line))\n","    \n","    #number of punctuations\n","    punctuation = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n","    num_punc = []\n","    \n","    for i in range(len(original)):\n","        line = original[i]\n","        try:\n","            if math.isnan(line):\n","                line = ''\n","        except:\n","            type(line) == str\n","        sentence = str(line)\n","        punc = 0\n","        for word in sentence:\n","            if word in punctuation:\n","                punc += 1\n","        num_punc.append(punc)\n","        \n","    #number of digits\n","    digits = '''1234567890'''\n","    num_digits = []\n","    \n","    for i in range(len(original)):\n","        line = original[i]\n","        try:\n","            if math.isnan(line):\n","                line = ''\n","        except:\n","            type(line) == str\n","        sentence = str(line)\n","        digit = 0\n","        for word in sentence:\n","            if word in digits:\n","                digit += 1\n","        num_digits.append(digit)\n","        \n","    #sentiment\n","    text_detokenized = data['Sentence'].apply(TreebankWordDetokenizer().detokenize)\n","    sia = SentimentIntensityAnalyzer()\n","    \n","    polarity = []\n","    for i in range(len(data['Sentence'])):\n","        polarity.append(sia.polarity_scores(text_detokenized.iloc[i]))\n","\n","\n","    polarity_score = polarity\n","    \n","    \n","    #add all the features\n","    data['pos__sentence']=pos__sentence\n","    data['len_sentence']=len_sentence\n","    data['num_punc']=num_punc\n","    data['num_digits']=num_digits\n","    data['polarity_score']=polarity_score\n","\n","    data.to_csv(outputs_path + '/labels/' + fname[:-4]+'.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8PPL3vywLpV5"},"source":["**Run Code cell below once all emails are annotated**"]},{"cell_type":"code","metadata":{"id":"I4R4YNaTLoe9"},"source":["all_labeled = pd.read_csv(outputs_path + '/labels/' + os.listdir(outputs_path + '/labels/')[0])\n","for f in os.listdir(outputs_path + '/labels/')[1:]:\n","    all_labeled = all_labeled.append(pd.read_csv(outputs_path + '/labels/' + f))\n","all_labeled.to_csv(outputs_path + '/all_labeled_data.csv', index=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e_MRaXh7ONXB"},"source":["## Creating and Training Model"]},{"cell_type":"markdown","metadata":{"id":"LBROfi4sWYXH"},"source":["### Loading Data into Tensorflow to be trained"]},{"cell_type":"code","metadata":{"id":"6IwI_2bcIeX8"},"source":["\n","seed = 42\n","\n","ds = tf.data.experimental.make_csv_dataset(\n","    outputs_path + '/all_labeled_data.csv',\n","    batch_size = 5,\n","    label_name='Label',\n","    num_epochs = 1,\n","    ignore_errors=True)\n","\n","non_train_ds = ds.take(2000)\n","train_ds = ds.skip(2000)\n","val_ds = non_train_ds.take(1000)\n","test_ds = non_train_ds.skip(1000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dX8FtlpGJRE6"},"source":["### Loading models from TensorFlow Hub\n","\n","Here you can choose which BERT model you will load from TensorFlow Hub and fine-tune. There are multiple BERT models available.\n","\n","  - [BERT-Base](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3), [Uncased](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3) and [seven more models](https://tfhub.dev/google/collections/bert/1) with trained weights released by the original BERT authors.\n","  - [Small BERTs](https://tfhub.dev/google/collections/bert/1) have the same general architecture but fewer and/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.\n","  - [ALBERT](https://tfhub.dev/google/collections/albert/1): four different sizes of \"A Lite BERT\" that reduces model size (but not computation time) by sharing parameters between layers.\n","  - [BERT Experts](https://tfhub.dev/google/collections/experts/bert/1): eight models that all have the BERT-base architecture but offer a choice between different pre-training domains, to align more closely with the target task.\n","  - [Electra](https://tfhub.dev/google/collections/electra/1) has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).\n","  - BERT with Talking-Heads Attention and Gated GELU [[base](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1), [large](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1)] has two improvements to the core of the Transformer architecture.\n","\n","The model documentation on TensorFlow Hub has more details and references to the\n","research literature. Follow the links above, or click on the [`tfhub.dev`](http://tfhub.dev) URL\n","printed after the next cell execution.\n","\n","The suggestion is to start with a Small BERT (with fewer parameters) since they are faster to fine-tune. If you like a small model but with higher accuracy, ALBERT might be your next option. If you want even better accuracy, choose\n","one of the classic BERT sizes or their recent refinements like Electra, Talking Heads, or a BERT Expert.\n","\n","Aside from the models available below, there are [multiple versions](https://tfhub.dev/google/collections/transformer_encoders_text/1) of the models that are larger and can yield even better accuracy, but they are too big to be fine-tuned on a single GPU. You will be able to do that on the [Solve GLUE tasks using BERT on a TPU colab](https://www.tensorflow.org/tutorials/text/solve_glue_tasks_using_bert_on_tpu).\n","\n","You'll see in the code below that switching the tfhub.dev URL is enough to try any of these models, because all the differences between them are encapsulated in the SavedModels from TF Hub."]},{"cell_type":"code","metadata":{"id":"y8_ctG55-uTX","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","executionInfo":{"status":"ok","timestamp":1619839820089,"user_tz":420,"elapsed":571,"user":{"displayName":"Andrew Sue","photoUrl":"","userId":"12996359075612494384"}},"outputId":"551fb657-62b3-4cc2-8d92-deb2d31fb119"},"source":["#@title Choose a BERT model to fine-tune\n","\n","bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n","\n","map_name_to_handle = {\n","    'bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n","    'bert_en_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n","    'bert_multi_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n","    'albert_en_base':\n","        'https://tfhub.dev/tensorflow/albert_en_base/2',\n","    'electra_small':\n","        'https://tfhub.dev/google/electra_small/2',\n","    'electra_base':\n","        'https://tfhub.dev/google/electra_base/2',\n","    'experts_pubmed':\n","        'https://tfhub.dev/google/experts/bert/pubmed/2',\n","    'experts_wiki_books':\n","        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n","    'talking-heads_base':\n","        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n","}\n","\n","map_model_to_preprocess = {\n","    'bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'bert_en_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'bert_multi_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n","    'albert_en_base':\n","        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n","    'electra_small':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'electra_base':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'experts_pubmed':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'experts_wiki_books':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'talking-heads_base':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","}\n","\n","tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n","tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n","\n","print(f'BERT model selected           : {tfhub_handle_encoder}')\n","print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0SQi-jWd_jzq"},"source":["bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n","text_preprocessed = bert_preprocess_model(text_test)\n","bert_model = hub.KerasLayer(tfhub_handle_encoder)\n","bert_results = bert_model(text_preprocessed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pDNKfAXbDnJH"},"source":["### Define your model\n","\n","You will create a very simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer.\n","\n","Note: for more information about the base model's input and output you can use just follow the model's url for documentation. Here specifically you don't need to worry about it because the preprocessing model will take care of that for you.\n"]},{"cell_type":"code","metadata":{"id":"aksj743St9ga"},"source":["def build_classifier_model():\n","  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='Sentence')\n","  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n","  encoder_inputs = preprocessing_layer(text_input)\n","  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n","  outputs = encoder(encoder_inputs)\n","  net = outputs['pooled_output']\n","  net = tf.keras.layers.Dropout(0.1)(net)\n","  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n","  return tf.keras.Model(text_input, net)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mGMF8AZcB2Zy"},"source":["classifier_model = build_classifier_model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OWPOZE-L3AgE"},"source":["# Define loss function\n","loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","metrics = tf.metrics.BinaryAccuracy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9eP2y9dbw32"},"source":["# Define optimizer\n","epochs = 5\n","steps_per_epoch = 300\n","num_train_steps = steps_per_epoch * epochs\n","num_warmup_steps = int(0.1*num_train_steps)\n","\n","init_lr = 3e-5\n","optimizer = optimization.create_optimizer(init_lr=init_lr,\n","                                          num_train_steps=num_train_steps,\n","                                          num_warmup_steps=num_warmup_steps,\n","                                          optimizer_type='adamw')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iTbqMZVjtyyU"},"source":["### Training the model"]},{"cell_type":"code","metadata":{"id":"-7GPDhR98jsD"},"source":["classifier_model.compile(optimizer=optimizer,\n","                         loss=loss,\n","                         metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HtfDFAnN_Neu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619579259510,"user_tz":420,"elapsed":391589,"user":{"displayName":"Andrew Sue","photoUrl":"","userId":"12996359075612494384"}},"outputId":"9fe1be3d-c2f8-408b-e789-def3fd47b081"},"source":["print(f'Training model with {tfhub_handle_encoder}')\n","history = classifier_model.fit(x=train_ds,\n","                               validation_data=val_ds,\n","                               epochs=5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training model with https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n","Epoch 1/5\n","162/162 [==============================] - 91s 472ms/step - loss: 0.6245 - binary_accuracy: 0.5265 - val_loss: 0.3260 - val_binary_accuracy: 0.8742\n","Epoch 2/5\n","162/162 [==============================] - 75s 463ms/step - loss: 0.4025 - binary_accuracy: 0.8883 - val_loss: 0.3618 - val_binary_accuracy: 0.8722\n","Epoch 3/5\n","162/162 [==============================] - 75s 464ms/step - loss: 0.3588 - binary_accuracy: 0.8887 - val_loss: 0.3926 - val_binary_accuracy: 0.8926\n","Epoch 4/5\n","162/162 [==============================] - 75s 465ms/step - loss: 0.3340 - binary_accuracy: 0.9051 - val_loss: 0.3640 - val_binary_accuracy: 0.8998\n","Epoch 5/5\n","162/162 [==============================] - 75s 463ms/step - loss: 0.2617 - binary_accuracy: 0.9224 - val_loss: 0.3303 - val_binary_accuracy: 0.9034\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uBthMlTSV8kn"},"source":["### Evaluate the model\n","\n","Let's see how the model performs. Two values will be returned. Loss (a number which represents the error, lower values are better), and accuracy."]},{"cell_type":"code","metadata":{"id":"slqB-urBV9sP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619579308101,"user_tz":420,"elapsed":48583,"user":{"displayName":"Andrew Sue","photoUrl":"","userId":"12996359075612494384"}},"outputId":"0599fa4c-ccb2-4f39-a0b2-247b21bacab2"},"source":["loss, accuracy = classifier_model.evaluate(test_ds)\n","\n","print(f'Loss: {loss}')\n","print(f'Accuracy: {accuracy}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1000/1000 [==============================] - 48s 48ms/step - loss: 0.3180 - binary_accuracy: 0.9040\n","Loss: 0.31804439425468445\n","Accuracy: 0.9039999842643738\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Rtn7jewb6dg4"},"source":["### Export for inference\n","\n","Now you just save your fine-tuned model for later use.\n","- Define the name of your model through setting ```dataset_name```\n","- Define the path where you would save your model by changing ```saved_model_path```\n","\n","More info, visit https://www.tensorflow.org/tutorials/keras/save_and_load\n","\n"]},{"cell_type":"code","metadata":{"id":"ShcvqJAgVera"},"source":["dataset_name = 'DSDP'\n","saved_model_path = './Models/{}_bert'.format(dataset_name.replace('/', '_'))\n","\n","# Saved model format\n","classifier_model.save(saved_model_path)\n","\n","# HDF5 format\n","# classifier_model.save('./'+saved_model_path+'.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oyTappHTvNCz"},"source":["Here you can test your model on any sentence you want, just add to the examples variable below too see what is a good threshold."]},{"cell_type":"code","metadata":{"id":"VBWzH6exlCPS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617807899176,"user_tz":420,"elapsed":3438,"user":{"displayName":"Andrew Sue","photoUrl":"","userId":"12996359075612494384"}},"outputId":"d55ea5b6-80a6-44f5-9b58-15da6eed5fe7"},"source":["def print_my_examples(inputs, results):\n","  result_for_printing = \\\n","    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n","                         for i in range(len(inputs))]\n","  print(*result_for_printing, sep='\\n')\n","  print()\n","\n","\n","examples = [\n","    'That is too much work!!!',\n","    'However, my leg is feeling better, thank you.',\n","    'Howis your day going?',\n","    'Message sent by:> eric (ebass@enron.com)>',\n","    '>',\n","    'AAA',\n","    'Encrypted message:>> QTDHF AYTPE KHYYN WSJYE TBXNQ PJRRH GDVDX QHNNK QBKOD ZVQEU LTRMQ WDHURACFGZ VJDPV UFIZA M',\n","    ', visit http://www.bestfares.com/view.asp?id=3D10106401FLY BETWEEN THE MIDWEST AND EAST COAST FOR $198 TO $218',\n","    'Thank You for Your CooperationRT'\n","\n","]\n","\n","original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n","print('Results from the model in memory:')\n","print_my_examples(examples, original_results)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Results from the saved model:\n","input: That is too much work!!!       : score: 0.999104\n","input: However, my leg is feeling better, thank you. : score: 0.999778\n","input: Howis your day going?          : score: 0.996103\n","input: Message sent by:> eric (ebass@enron.com)> : score: 0.010370\n","input: >                              : score: 0.922872\n","input: AAA                            : score: 0.646321\n","input: Encrypted message:>> QTDHF AYTPE KHYYN WSJYE TBXNQ PJRRH GDVDX QHNNK QBKOD ZVQEU LTRMQ WDHURACFGZ VJDPV UFIZA M : score: 0.873582\n","input: , visit http://www.bestfares.com/view.asp?id=3D10106401FLY BETWEEN THE MIDWEST AND EAST COAST FOR $198 TO $218 : score: 0.008605\n","input: Thank You for Your CooperationRT : score: 0.652361\n","\n","Results from the model in memory:\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qcbtB_g9oXzU"},"source":["## Using Model to Clean Email"]},{"cell_type":"markdown","metadata":{"id":"cJBMnNKe4nBL"},"source":["### Instructions\n","1. Define where the inputs and outputs will reside: \n","    - ```outputs``` where the filtered outputs resides \n","    - ```inputs``` where all the raw input email files to cleaned resides \n","2. Define the path of model ```model_path``` to reload the model\n","2. Run the code below and check out the cleaned results in your ``outputs`` directory"]},{"cell_type":"code","metadata":{"id":"zDGb-dxd9DiT"},"source":["# model_path = saved_model_path\n","model_path = './Models/DSDP_bert'\n","reloaded_model = tf.saved_model.load(model_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1yZJEfzl4jAc"},"source":["# cleans all file in the inputs directory and filter using model\n","outputs = create_dir('./Filtered/arnold-j/')\n","inputs = './Inputs/arnold-j/'\n","for files in os.listdir(inputs):\n","    corpus = clean_summarize(inputs + files, files, annotation=False).splitlines()\n","\n","    # predicting\n","    try:\n","        reloaded_results = tf.sigmoid(reloaded_model(tf.constant(corpus)))\n","    except:\n","        print(files)\n","        print('The file above failed predicting.')\n","\n","    # join and filters result\n","    corpus = np.array(corpus)\n","    results = np.hstack((corpus.reshape(len(corpus), 1), reloaded_results.numpy()))\n","    results = np.array(list(filter(lambda a : float(a[1]) >= 0.8, results)))\n","    final_output = '\\n'.join(results[:, 0])\n","\n","    with open(outputs + files[:-4] + '.txt', \"w\") as new_file:\n","        new_file.write(final_output)\n"],"execution_count":null,"outputs":[]}]}